{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/preprocessed_aptos"
      ],
      "metadata": {
        "id": "8intviqTYp7_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# HÜCRE 1: PREPROCESSED DATASET OLUŞTUR (MULTI-THREAD) - AUTO PATH RESOLVER\n",
        "# ==========================\n",
        "import os, glob, warnings, sys\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --------------------------\n",
        "# CONFIG\n",
        "# --------------------------\n",
        "SEED = 42\n",
        "TRAIN_FRACTION = 1.00   # debug: 0.20, full: 1.00\n",
        "VAL_FRACTION   = 1.00\n",
        "\n",
        "PREP_ROOT = \"/content/preprocessed_aptos\"\n",
        "PREP_TRAIN_DIR = os.path.join(PREP_ROOT, \"train_images\")\n",
        "PREP_VAL_DIR   = os.path.join(PREP_ROOT, \"val_images\")\n",
        "PREP_TEST_DIR  = os.path.join(PREP_ROOT, \"test_images\")\n",
        "\n",
        "IMG_EXTS = (\".png\", \".jpg\", \".jpeg\")\n",
        "\n",
        "# OpenCV thread kapat (ThreadPool ile daha stabil)\n",
        "try:\n",
        "    cv2.setNumThreads(0)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# --------------------------\n",
        "# 0) Drive mount (Colab ise)\n",
        "# --------------------------\n",
        "def try_mount_drive():\n",
        "    if os.path.exists(\"/content/drive\"):\n",
        "        return\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/content/drive\")\n",
        "    except Exception as e:\n",
        "        print(\" Drive mount edilemedi (Colab dışında olabilir):\", e)\n",
        "\n",
        "try_mount_drive()\n",
        "\n",
        "# Arama kökleri\n",
        "SEARCH_ROOTS = []\n",
        "for r in [\"/content/drive/MyDrive\", \"/content/drive\", \"/content\"]:\n",
        "    if os.path.exists(r):\n",
        "        SEARCH_ROOTS.append(r)\n",
        "\n",
        "print(\"SEARCH_ROOTS:\", SEARCH_ROOTS)\n",
        "\n",
        "# --------------------------\n",
        "# 1) CSV'leri otomatik\n",
        "# --------------------------\n",
        "def find_file_candidates(filename, roots, limit=10):\n",
        "    hits = []\n",
        "    for root in roots:\n",
        "        pat = os.path.join(root, \"**\", filename)\n",
        "        hits.extend(glob.glob(pat, recursive=True))\n",
        "\n",
        "    hits = sorted(list(dict.fromkeys(hits)))\n",
        "    return hits[:limit]\n",
        "\n",
        "def pick_best_csv(name, candidates):\n",
        "\n",
        "    return candidates[0] if candidates else None\n",
        "\n",
        "\n",
        "expected_csvs = {\n",
        "    \"TRAIN_CSV\": [\"train_1.csv\", \"train.csv\"],\n",
        "    \"VAL_CSV\"  : [\"valid.csv\", \"val.csv\", \"validation.csv\"],\n",
        "    \"TEST_CSV\" : [\"test.csv\"]\n",
        "}\n",
        "\n",
        "resolved_csv = {}\n",
        "all_candidates_debug = {}\n",
        "\n",
        "for key, names in expected_csvs.items():\n",
        "    cands = []\n",
        "    for nm in names:\n",
        "        c = find_file_candidates(nm, SEARCH_ROOTS, limit=20)\n",
        "        if c:\n",
        "            cands.extend(c)\n",
        "    cands = sorted(list(dict.fromkeys(cands)))\n",
        "    all_candidates_debug[key] = cands\n",
        "    resolved_csv[key] = pick_best_csv(key, cands)\n",
        "\n",
        "print(\"\\n CSV candidates (first 20 each):\")\n",
        "for k, cands in all_candidates_debug.items():\n",
        "    print(f\" - {k}: {len(cands)} found\")\n",
        "    for x in cands[:5]:\n",
        "        print(\"    \", x)\n",
        "\n",
        "TRAIN_CSV = resolved_csv[\"TRAIN_CSV\"]\n",
        "VAL_CSV   = resolved_csv[\"VAL_CSV\"]\n",
        "TEST_CSV  = resolved_csv[\"TEST_CSV\"]\n",
        "\n",
        "if not (TRAIN_CSV and VAL_CSV and TEST_CSV):\n",
        "    raise FileNotFoundError(\n",
        "        \" CSV bulunamadı.\\n\"\n",
        "        \" Çözüm: Drive’da dosyaların gerçekten var olduğundan emin ol.\\n\"\n",
        "        \"   - train_1.csv / valid.csv / test.csv dosyalarını Drive’a yükle\\n\"\n",
        "        \"   - ya da dosya adların farklıysa burada bulunan aday isimlere göre güncelle.\\n\"\n",
        "        \"Not: Yukarıda 'CSV candidates' altında bulunan path’lerden biri doğru olmalı.\"\n",
        "    )\n",
        "\n",
        "print(\"\\n RESOLVED CSV PATHS:\")\n",
        "print(\"TRAIN_CSV:\", TRAIN_CSV)\n",
        "print(\"VAL_CSV  :\", VAL_CSV)\n",
        "print(\"TEST_CSV :\", TEST_CSV)\n",
        "\n",
        "# --------------------------\n",
        "# 2) CSV'leri oku\n",
        "# --------------------------\n",
        "df_train = pd.read_csv(TRAIN_CSV)\n",
        "df_val   = pd.read_csv(VAL_CSV)\n",
        "df_test  = pd.read_csv(TEST_CSV)\n",
        "\n",
        "for name, df in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
        "    if \"id_code\" not in df.columns:\n",
        "        raise ValueError(f\" {name} csv içinde 'id_code' kolonu yok!\")\n",
        "print(\" CSV read OK. Columns:\", df_train.columns.tolist())\n",
        "\n",
        "# diagnosis varsa string yap\n",
        "if \"diagnosis\" in df_train.columns:\n",
        "    df_train[\"diagnosis\"] = df_train[\"diagnosis\"].astype(str)\n",
        "if \"diagnosis\" in df_val.columns:\n",
        "    df_val[\"diagnosis\"] = df_val[\"diagnosis\"].astype(str)\n",
        "\n",
        "# --------------------------\n",
        "\n",
        "def ensure_ext(series, ext):\n",
        "    s = series.astype(str)\n",
        "    return s.apply(lambda x: x if x.lower().endswith(IMG_EXTS) else (x + ext))\n",
        "\n",
        "def find_dir_by_filenames(roots, filenames, max_probe=50):\n",
        "    probe = [str(x) for x in filenames[:max_probe]]\n",
        "\n",
        "    for root in roots:\n",
        "        if not os.path.exists(root):\n",
        "            continue\n",
        "\n",
        "        # probe ile birebir eşleşme\n",
        "        for nm in probe:\n",
        "            if nm.lower().endswith(IMG_EXTS):\n",
        "                hits = glob.glob(os.path.join(root, \"**\", nm), recursive=True)\n",
        "                if hits:\n",
        "                    hit = hits[0]\n",
        "                    return os.path.dirname(hit), os.path.splitext(hit)[1].lower()\n",
        "            else:\n",
        "                for ext in IMG_EXTS:\n",
        "                    hits = glob.glob(os.path.join(root, \"**\", nm + ext), recursive=True)\n",
        "                    if hits:\n",
        "                        hit = hits[0]\n",
        "                        return os.path.dirname(hit), ext\n",
        "\n",
        "        # fallback: root altında herhangi bir resim var mı\n",
        "        any_imgs = []\n",
        "        for ext in IMG_EXTS:\n",
        "            any_imgs += glob.glob(os.path.join(root, \"**\", f\"*{ext}\"), recursive=True)\n",
        "        if any_imgs:\n",
        "            hit = any_imgs[0]\n",
        "            return os.path.dirname(hit), os.path.splitext(hit)[1].lower()\n",
        "\n",
        "    return None, None\n",
        "\n",
        "# id_code örnekleri (ext'siz olabilir)\n",
        "train_ids = df_train[\"id_code\"].astype(str).tolist()\n",
        "val_ids   = df_val[\"id_code\"].astype(str).tolist()\n",
        "test_ids  = df_test[\"id_code\"].astype(str).tolist()\n",
        "\n",
        "TRAIN_DIR, ext1 = find_dir_by_filenames(SEARCH_ROOTS, train_ids)\n",
        "VAL_DIR,   ext2 = find_dir_by_filenames(SEARCH_ROOTS, val_ids)\n",
        "TEST_DIR,  ext3 = find_dir_by_filenames(SEARCH_ROOTS, test_ids)\n",
        "\n",
        "# APTOS gibi yapılarda val ayrı klasör olmayabilir → train'e düş\n",
        "if VAL_DIR is None and TRAIN_DIR is not None:\n",
        "    VAL_DIR, ext2 = TRAIN_DIR, ext1\n",
        "\n",
        "if TEST_DIR is None and TRAIN_DIR is not None:\n",
        "    # bazı senaryolarda test de aynı yerde olabilir\n",
        "    TEST_DIR, ext3 = TRAIN_DIR, ext1\n",
        "\n",
        "print(\"\\n RESOLVED IMAGE DIRS:\")\n",
        "print(\"TRAIN_DIR:\", TRAIN_DIR, \"ext:\", ext1)\n",
        "print(\"VAL_DIR  :\", VAL_DIR,   \"ext:\", ext2)\n",
        "print(\"TEST_DIR :\", TEST_DIR,  \"ext:\", ext3)\n",
        "\n",
        "if TRAIN_DIR is None:\n",
        "    raise FileNotFoundError(\" TRAIN resimleri bulunamadı. Drive içinde train_images klasörü/PNG'ler yok gibi.\")\n",
        "if TEST_DIR is None:\n",
        "    raise FileNotFoundError(\" TEST resimleri bulunamadı. Drive içinde test images yok gibi.\")\n",
        "\n",
        "IMAGE_EXT = ext1 or ext2 or ext3 or \".png\"\n",
        "print(\" Using IMAGE_EXT:\", IMAGE_EXT)\n",
        "\n",
        "\n",
        "df_train[\"id_code\"] = ensure_ext(df_train[\"id_code\"], IMAGE_EXT)\n",
        "df_val[\"id_code\"]   = ensure_ext(df_val[\"id_code\"],   IMAGE_EXT)\n",
        "df_test[\"id_code\"]  = ensure_ext(df_test[\"id_code\"],  IMAGE_EXT)\n",
        "\n",
        "# --------------------------\n",
        "# 4) % subset (train/val)\n",
        "# --------------------------\n",
        "def apply_fraction(df, frac, label_col=None):\n",
        "    if frac >= 1.0:\n",
        "        return df.reset_index(drop=True)\n",
        "    if label_col and (label_col in df.columns):\n",
        "        try:\n",
        "            small, _ = train_test_split(\n",
        "                df,\n",
        "                test_size=(1.0-frac),\n",
        "                random_state=SEED,\n",
        "                stratify=df[label_col]\n",
        "            )\n",
        "            return small.reset_index(drop=True)\n",
        "        except Exception:\n",
        "            return df.sample(frac=frac, random_state=SEED).reset_index(drop=True)\n",
        "    return df.sample(frac=frac, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "df_train_small = apply_fraction(df_train, TRAIN_FRACTION, label_col=\"diagnosis\" if \"diagnosis\" in df_train.columns else None)\n",
        "df_val_small   = apply_fraction(df_val,   VAL_FRACTION,   label_col=\"diagnosis\" if \"diagnosis\" in df_val.columns else None)\n",
        "\n",
        "print(\"\\n Using train:\", len(df_train_small), \"/\", len(df_train))\n",
        "print(\" Using val  :\", len(df_val_small),   \"/\", len(df_val))\n",
        "print(\" Using test :\", len(df_test),        \"/\", len(df_test))\n",
        "\n",
        "# --------------------------\n",
        "# 5) Preprocess ops\n",
        "# --------------------------\n",
        "def crop_black_borders(img_bgr, tol=10):\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    mask = gray > tol\n",
        "    if mask.sum() == 0:\n",
        "        return img_bgr\n",
        "    coords = np.argwhere(mask)\n",
        "    y0, x0 = coords.min(axis=0)\n",
        "    y1, x1 = coords.max(axis=0) + 1\n",
        "    return img_bgr[y0:y1, x0:x1]\n",
        "\n",
        "def align_by_centroid(img_bgr, tol=10):\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    mask = (gray > tol).astype(np.uint8)\n",
        "    M = cv2.moments(mask)\n",
        "    if M[\"m00\"] == 0:\n",
        "        return img_bgr\n",
        "    cx = int(M[\"m10\"]/M[\"m00\"]); cy = int(M[\"m01\"]/M[\"m00\"])\n",
        "    h, w = img_bgr.shape[:2]\n",
        "    tx = (w//2) - cx; ty = (h//2) - cy\n",
        "    mat = np.float32([[1,0,tx],[0,1,ty]])\n",
        "    return cv2.warpAffine(img_bgr, mat, (w,h), flags=cv2.INTER_LINEAR,\n",
        "                          borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0))\n",
        "\n",
        "def clahe_green(img_bgr, clip=2.0, grid=(8,8)):\n",
        "    b,g,r = cv2.split(img_bgr)\n",
        "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=grid)\n",
        "    g2 = clahe.apply(g)\n",
        "    return cv2.merge([b,g2,r])\n",
        "\n",
        "def preprocess_bgr(img_bgr):\n",
        "    img_bgr = cv2.GaussianBlur(img_bgr, (3,3), 0)\n",
        "    img_bgr = crop_black_borders(img_bgr, tol=10)\n",
        "    img_bgr = align_by_centroid(img_bgr, tol=10)\n",
        "    img_bgr = clahe_green(img_bgr)\n",
        "    return img_bgr\n",
        "\n",
        "def build_preprocessed_dataset(df, src_dir, dst_dir, workers=8, out_size=(224,224), overwrite=False):\n",
        "    os.makedirs(dst_dir, exist_ok=True)\n",
        "    files = df[\"id_code\"].astype(str).tolist()\n",
        "\n",
        "    def _one(fname):\n",
        "        base = os.path.basename(fname)  # güvenlik: path gelirse basename al\n",
        "        src = os.path.join(src_dir, base)\n",
        "        dst = os.path.join(dst_dir, base)\n",
        "\n",
        "        if (not overwrite) and os.path.exists(dst):\n",
        "            return \"skipped\"\n",
        "\n",
        "        img = cv2.imread(src)\n",
        "        if img is None:\n",
        "            return \"missing\"\n",
        "\n",
        "        img = preprocess_bgr(img)\n",
        "        img = cv2.resize(img, out_size, interpolation=cv2.INTER_AREA)\n",
        "        ok = cv2.imwrite(dst, img)\n",
        "        return \"ok\" if ok else \"failed\"\n",
        "\n",
        "    stats = {\"ok\":0, \"skipped\":0, \"missing\":0, \"failed\":0}\n",
        "    with ThreadPoolExecutor(max_workers=workers) as ex:\n",
        "        futs = [ex.submit(_one, f) for f in files]\n",
        "        for fu in tqdm(as_completed(futs), total=len(futs), desc=f\"Preprocess -> {os.path.basename(dst_dir)}\"):\n",
        "            stats[fu.result()] += 1\n",
        "\n",
        "    print(f\" Done {dst_dir} | {stats}\")\n",
        "    return stats\n",
        "\n",
        "\n",
        "\n",
        "_ = build_preprocessed_dataset(df_train_small, TRAIN_DIR, PREP_TRAIN_DIR, workers=8, overwrite=False)\n",
        "_ = build_preprocessed_dataset(df_val_small,   VAL_DIR,   PREP_VAL_DIR,   workers=8, overwrite=False)\n",
        "_ = build_preprocessed_dataset(df_test,        TEST_DIR,  PREP_TEST_DIR,  workers=8, overwrite=False)\n",
        "\n",
        "print(\"\\nPREP_ROOT:\", PREP_ROOT)\n",
        "print(\"Counts:\",\n",
        "      len(glob.glob(PREP_TRAIN_DIR+\"/*\")),\n",
        "      len(glob.glob(PREP_VAL_DIR+\"/*\")),\n",
        "      len(glob.glob(PREP_TEST_DIR+\"/*\")))\n",
        "\n",
        "print(\"\\n Ready: df_train_small, df_val_small, PREP_TRAIN_DIR, PREP_VAL_DIR, PREP_TEST_DIR\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8XzVD9wqE_i",
        "outputId": "0309742b-ac6e-40c1-a018-b42cae915acd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEARCH_ROOTS: ['/content/drive/MyDrive', '/content/drive', '/content']\n",
            "\n",
            " CSV candidates (first 20 each):\n",
            " - TRAIN_CSV: 1 found\n",
            "     /content/drive/MyDrive/Image Processing Project/train_1.csv\n",
            " - VAL_CSV: 1 found\n",
            "     /content/drive/MyDrive/Image Processing Project/valid.csv\n",
            " - TEST_CSV: 1 found\n",
            "     /content/drive/MyDrive/Image Processing Project/test.csv\n",
            "\n",
            " RESOLVED CSV PATHS:\n",
            "TRAIN_CSV: /content/drive/MyDrive/Image Processing Project/train_1.csv\n",
            "VAL_CSV  : /content/drive/MyDrive/Image Processing Project/valid.csv\n",
            "TEST_CSV : /content/drive/MyDrive/Image Processing Project/test.csv\n",
            " CSV read OK. Columns: ['id_code', 'diagnosis']\n",
            "\n",
            " RESOLVED IMAGE DIRS:\n",
            "TRAIN_DIR: /content/drive/MyDrive/Image Processing Project/train_images/train_images ext: .png\n",
            "VAL_DIR  : /content/drive/MyDrive/Image Processing Project/val_images/val_images ext: .png\n",
            "TEST_DIR : /content/drive/MyDrive/Image Processing Project/test_images/test_images ext: .png\n",
            " Using IMAGE_EXT: .png\n",
            "\n",
            " Using train: 2930 / 2930\n",
            " Using val  : 366 / 366\n",
            " Using test : 366 / 366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocess -> train_images: 100%|██████████| 2930/2930 [10:13<00:00,  4.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Done /content/preprocessed_aptos/train_images | {'ok': 2930, 'skipped': 0, 'missing': 0, 'failed': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocess -> val_images: 100%|██████████| 366/366 [01:20<00:00,  4.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Done /content/preprocessed_aptos/val_images | {'ok': 366, 'skipped': 0, 'missing': 0, 'failed': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocess -> test_images: 100%|██████████| 366/366 [01:08<00:00,  5.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Done /content/preprocessed_aptos/test_images | {'ok': 366, 'skipped': 0, 'missing': 0, 'failed': 0}\n",
            "\n",
            "PREP_ROOT: /content/preprocessed_aptos\n",
            "Counts: 2930 366 366\n",
            "\n",
            " Ready: df_train_small, df_val_small, PREP_TRAIN_DIR, PREP_VAL_DIR, PREP_TEST_DIR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"df_train_small\" in globals())\n",
        "print(\"df_val_small\" in globals())\n",
        "print(\"PREP_TRAIN_DIR\" in globals())\n",
        "print(\"PREP_VAL_DIR\" in globals())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMopY8_Y7CeU",
        "outputId": "290e4ece-c824-4ce4-8c61-e97cf0104a37"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# HÜCRE 2 (GECE ÇALIŞMAYA UYGUN) - TRAINING (İSTEDİĞİN ÇIKTILARLA)\n",
        "# - Checkpoint'ler DRIVE'a yazılır (best_model.keras + her epoch weights)\n",
        "# - Her epoch: progress satırı + QWK + Confusion Matrix (print + Drive'a kaydet)\n",
        "# - preprocess_input\n",
        "# - class mapping fix\n",
        "# - EarlyStopping + \"Restoring model weights...\" çıktısı\n",
        "# - FINAL: QWK + prediction distribution + confusion matrix\n",
        "# ==========================\n",
        "\n",
        "import os, re, glob, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(\"TF:\", tf.__version__)\n",
        "print(\"GPU:\", tf.config.list_physical_devices(\"GPU\"))\n",
        "\n",
        "# --------------------------\n",
        "# 0) Drive mount (Colab)\n",
        "# --------------------------\n",
        "def try_mount_drive():\n",
        "    if os.path.exists(\"/content/drive\"):\n",
        "        return\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/content/drive\")\n",
        "    except Exception as e:\n",
        "        print(\" Drive mount edilemedi:\", e)\n",
        "\n",
        "try_mount_drive()\n",
        "\n",
        "# --------------------------\n",
        "# 1) Bu hücrenin beklediği değişkenler (Hücre 1’den gelir)\n",
        "# --------------------------\n",
        "needed = [\"df_train_small\", \"df_val_small\", \"PREP_TRAIN_DIR\", \"PREP_VAL_DIR\"]\n",
        "missing = [n for n in needed if n not in globals()]\n",
        "if missing:\n",
        "    raise RuntimeError(f\" Hücre 1 çalışmadan bu hücre çalışmaz. Eksikler: {missing}\")\n",
        "\n",
        "# --------------------------\n",
        "# 2) Ayarlar\n",
        "# --------------------------\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "IMG_SIZE   = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "WARMUP_EPOCHS = 3\n",
        "FT_EPOCHS     = 10\n",
        "FT_LR         = 1e-4\n",
        "WARMUP_LR     = 1e-3\n",
        "\n",
        "NUM_CLASSES = 5\n",
        "CLASS_NAMES = [str(i) for i in range(NUM_CLASSES)]\n",
        "CLASS_TO_IDX = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
        "\n",
        "# --------------------------\n",
        "# 3) Checkpoint & Log klasörleri\n",
        "# --------------------------\n",
        "BASE_DRIVE = \"/content/drive/MyDrive/Image Processing Project\"\n",
        "CKPT_DIR   = os.path.join(BASE_DRIVE, \"checkpoints\")\n",
        "LOG_DIR    = os.path.join(BASE_DRIVE, \"logs_night\")\n",
        "CM_DIR     = os.path.join(BASE_DRIVE, \"confusion_matrices_night\")\n",
        "\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "os.makedirs(CM_DIR, exist_ok=True)\n",
        "\n",
        "BEST_MODEL_PATH = os.path.join(CKPT_DIR, \"best_model.keras\")  #\n",
        "EPOCH_WEIGHTS_PATH = os.path.join(CKPT_DIR, \"epoch_{epoch:03d}_valloss_{val_loss:.5f}.weights.h5\")\n",
        "\n",
        "print(\" CKPT_DIR:\", CKPT_DIR)\n",
        "print(\" BEST_MODEL:\", BEST_MODEL_PATH)\n",
        "print(\" CM_DIR:\", CM_DIR)\n",
        "\n",
        "# --------------------------\n",
        "# 4) Class mapping fix + path hazırlama\n",
        "# --------------------------\n",
        "def fix_label_series(s):\n",
        "    s = s.astype(str).str.strip()\n",
        "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)     # \"2.0\" -> \"2\"\n",
        "    s = s.where(s.isin(CLASS_NAMES), other=np.nan) # sadece 0..4 kalsın\n",
        "    return s\n",
        "\n",
        "df_train_small = df_train_small.copy()\n",
        "df_val_small   = df_val_small.copy()\n",
        "\n",
        "if \"diagnosis\" not in df_train_small.columns or \"diagnosis\" not in df_val_small.columns:\n",
        "    raise ValueError(\" df_train_small / df_val_small içinde 'diagnosis' yok!\")\n",
        "\n",
        "df_train_small[\"diagnosis_fixed\"] = fix_label_series(df_train_small[\"diagnosis\"])\n",
        "df_val_small[\"diagnosis_fixed\"]   = fix_label_series(df_val_small[\"diagnosis\"])\n",
        "\n",
        "before_tr, before_va = len(df_train_small), len(df_val_small)\n",
        "df_train_small = df_train_small.dropna(subset=[\"diagnosis_fixed\"]).reset_index(drop=True)\n",
        "df_val_small   = df_val_small.dropna(subset=[\"diagnosis_fixed\"]).reset_index(drop=True)\n",
        "print(f\" Train label cleaned: {before_tr} -> {len(df_train_small)}\")\n",
        "print(f\" Val label cleaned  : {before_va} -> {len(df_val_small)}\")\n",
        "\n",
        "df_train_small[\"label\"] = df_train_small[\"diagnosis_fixed\"].map(CLASS_TO_IDX).astype(int)\n",
        "df_val_small[\"label\"]   = df_val_small[\"diagnosis_fixed\"].map(CLASS_TO_IDX).astype(int)\n",
        "\n",
        "def make_paths(df, img_dir):\n",
        "    names = df[\"id_code\"].astype(str).apply(lambda x: os.path.basename(x)).tolist()\n",
        "    return [os.path.join(img_dir, n) for n in names]\n",
        "\n",
        "train_paths = make_paths(df_train_small, PREP_TRAIN_DIR)\n",
        "val_paths   = make_paths(df_val_small,   PREP_VAL_DIR)\n",
        "train_labels = df_train_small[\"label\"].values.astype(np.int32)\n",
        "val_labels   = df_val_small[\"label\"].values.astype(np.int32)\n",
        "\n",
        "def filter_existing(paths, labels):\n",
        "    keep_p, keep_y = [], []\n",
        "    missing = 0\n",
        "    for p,y in zip(paths, labels):\n",
        "        if os.path.exists(p):\n",
        "            keep_p.append(p); keep_y.append(y)\n",
        "        else:\n",
        "            missing += 1\n",
        "    return keep_p, np.array(keep_y, dtype=np.int32), missing\n",
        "\n",
        "train_paths, train_labels, miss_tr = filter_existing(train_paths, train_labels)\n",
        "val_paths,   val_labels,   miss_va = filter_existing(val_paths,   val_labels)\n",
        "\n",
        "print(\" Train files:\", len(train_paths), \"| missing:\", miss_tr)\n",
        "print(\" Val files  :\", len(val_paths),   \"| missing:\", miss_va)\n",
        "if len(train_paths) == 0 or len(val_paths) == 0:\n",
        "    raise RuntimeError(\" Train/Val path boş. PREP_TRAIN_DIR / PREP_VAL_DIR içinde dosya yok gibi.\")\n",
        "\n",
        "# --------------------------\n",
        "# 5) tf.data pipeline (preprocess_input + one-hot labels)\n",
        "# --------------------------\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def decode_and_preprocess(path, label, training=False):\n",
        "    img_bytes = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False)\n",
        "    img = tf.image.resize(img, IMG_SIZE, method=\"bilinear\")\n",
        "    img = tf.cast(img, tf.float32)\n",
        "\n",
        "    if training:\n",
        "        img = tf.image.random_flip_left_right(img)\n",
        "        img = tf.image.random_brightness(img, 0.05)\n",
        "\n",
        "    img = preprocess_input(img)  # preprocess_input\n",
        "    y = tf.one_hot(label, NUM_CLASSES)  #  AUC + top2_acc için\n",
        "    return img, y\n",
        "\n",
        "def make_ds(paths, labels, training=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "    if training:\n",
        "        ds = ds.shuffle(min(2000, len(paths)), seed=SEED, reshuffle_each_iteration=True)\n",
        "    ds = ds.map(lambda p,y: decode_and_preprocess(p,y,training=training), num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_ds(train_paths, train_labels, training=True)\n",
        "val_ds   = make_ds(val_paths,   val_labels,   training=False)\n",
        "\n",
        "# --------------------------\n",
        "# 6) Model\n",
        "# --------------------------\n",
        "def build_model():\n",
        "    base = EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=(*IMG_SIZE, 3))\n",
        "    base.trainable = False  # warmup: frozen\n",
        "\n",
        "    inp = keras.Input(shape=(*IMG_SIZE, 3))\n",
        "    x = base(inp, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inp, out)\n",
        "    return model, base\n",
        "\n",
        "model, base = build_model()\n",
        "\n",
        "metrics = [\n",
        "    keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "    keras.metrics.AUC(name=\"auc\", multi_label=True, num_labels=NUM_CLASSES),\n",
        "    keras.metrics.TopKCategoricalAccuracy(k=2, name=\"top2_acc\"),\n",
        "]\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(WARMUP_LR),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# 7) Resume (varsa en son epoch weights’ten devam)\n",
        "# --------------------------\n",
        "def get_latest_epoch_ckpt(ckpt_dir):\n",
        "    files = glob.glob(os.path.join(ckpt_dir, \"epoch_*.weights.h5\"))\n",
        "    if not files:\n",
        "        return None, 0\n",
        "    def epoch_num(f):\n",
        "        m = re.search(r\"epoch_(\\d+)_\", os.path.basename(f))\n",
        "        return int(m.group(1)) if m else -1\n",
        "    files = sorted(files, key=epoch_num)\n",
        "    latest = files[-1]\n",
        "    return latest, epoch_num(latest)\n",
        "\n",
        "latest_ckpt, last_epoch = get_latest_epoch_ckpt(CKPT_DIR)\n",
        "initial_epoch = 0\n",
        "if latest_ckpt:\n",
        "    print(f\" Resume: {latest_ckpt} (epoch={last_epoch})\")\n",
        "    model.load_weights(latest_ckpt)\n",
        "    initial_epoch = last_epoch\n",
        "else:\n",
        "    print(\" Resume checkpoint yok, sıfırdan başlanıyor.\")\n",
        "\n",
        "# --------------------------\n",
        "# 8) Callback: Epoch progress + QWK + Confusion Matrix (print + kaydet)\n",
        "# --------------------------\n",
        "class EpochProgressQWKCM(keras.callbacks.Callback):\n",
        "    def __init__(self, val_ds, class_names, cm_dir, total_epochs):\n",
        "        super().__init__()\n",
        "        self.val_ds = val_ds\n",
        "        self.class_names = class_names\n",
        "        self.cm_dir = cm_dir\n",
        "        self.total_epochs = total_epochs\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        # epoch index 0-based\n",
        "        pct = (epoch / max(1, self.total_epochs)) * 100.0\n",
        "        print(f\"\\n Epoch progress: {epoch+1}/{self.total_epochs} ({pct:.1f}%)\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # y_true / y_pred\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "\n",
        "        for xb, yb_onehot in self.val_ds:\n",
        "            pr = self.model.predict(xb, verbose=0)\n",
        "            y_true.extend(np.argmax(yb_onehot.numpy(), axis=1).tolist())\n",
        "            y_pred.extend(np.argmax(pr, axis=1).tolist())\n",
        "\n",
        "        qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
        "        if logs is not None:\n",
        "            logs[\"val_qwk\"] = qwk\n",
        "\n",
        "        print(f\"✅ QWK (val): {qwk:.4f}\")\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=list(range(len(self.class_names))))\n",
        "        print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "        # Kaydet (png + csv)\n",
        "        out_png = os.path.join(self.cm_dir, f\"cm_epoch_{epoch+1:03d}.png\")\n",
        "        out_csv = os.path.join(self.cm_dir, f\"cm_epoch_{epoch+1:03d}.csv\")\n",
        "\n",
        "        # png\n",
        "        fig = plt.figure(figsize=(6,5))\n",
        "        plt.imshow(cm, interpolation=\"nearest\")\n",
        "        plt.title(f\"Confusion Matrix (epoch {epoch+1})\")\n",
        "        plt.colorbar()\n",
        "        ticks = np.arange(len(self.class_names))\n",
        "        plt.xticks(ticks, self.class_names)\n",
        "        plt.yticks(ticks, self.class_names)\n",
        "        thresh = cm.max()/2.0 if cm.max() > 0 else 0.5\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                plt.text(j, i, int(cm[i,j]),\n",
        "                         ha=\"center\", va=\"center\",\n",
        "                         color=\"white\" if cm[i,j] > thresh else \"black\")\n",
        "        plt.ylabel(\"True\")\n",
        "        plt.xlabel(\"Pred\")\n",
        "        plt.tight_layout()\n",
        "        fig.savefig(out_png, dpi=150)\n",
        "        plt.close(fig)\n",
        "\n",
        "        # csv\n",
        "        pd.DataFrame(cm, index=self.class_names, columns=self.class_names).to_csv(out_csv)\n",
        "\n",
        "# --------------------------\n",
        "# 9) Callbacks: best_model.keras + epoch weights + early stop + LR\n",
        "# --------------------------\n",
        "cb_best = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=BEST_MODEL_PATH,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,   #  best_model.keras\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "cb_epoch = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=EPOCH_WEIGHTS_PATH,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    save_best_only=False,\n",
        "    save_weights_only=True,    #  her epoch weights\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "cb_csv = keras.callbacks.CSVLogger(os.path.join(LOG_DIR, \"history.csv\"), append=True)\n",
        "\n",
        "cb_rlr = keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "cb_es = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True,  #  \"Restoring model weights...\" çıktısı\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# WARMUP + FINETUNE için ayrı total epoch hesapları (progress satırı için)\n",
        "cb_qwk_cm_warm = EpochProgressQWKCM(val_ds, CLASS_NAMES, CM_DIR, total_epochs=WARMUP_EPOCHS)\n",
        "cb_qwk_cm_ft   = EpochProgressQWKCM(val_ds, CLASS_NAMES, CM_DIR, total_epochs=FT_EPOCHS)\n",
        "\n",
        "# --------------------------\n",
        "# 10) TRAIN: WARMUP\n",
        "# --------------------------\n",
        "print(\"\\n=== WARMUP (base frozen) ===\")\n",
        "\n",
        "hist_warm = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=WARMUP_EPOCHS,\n",
        "    initial_epoch=min(initial_epoch, WARMUP_EPOCHS),\n",
        "    callbacks=[cb_qwk_cm_warm, cb_best, cb_epoch, cb_csv, cb_rlr, cb_es],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# 11) TRAIN: FINE-TUNE\n",
        "# --------------------------\n",
        "print(\"\\n=== FINE-TUNE (base trainable=True) ===\")\n",
        "\n",
        "# base trainable\n",
        "base.trainable = True\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(FT_LR),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "# fine-tune için early stopping'i yeniden başlatmak için yeni callback oluşturulur\n",
        "cb_es_ft = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "hist_ft = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=FT_EPOCHS,\n",
        "    initial_epoch=0,  # fine-tune kendi içinde 1/10 şeklinde yazsın\n",
        "    callbacks=[cb_qwk_cm_ft, cb_best, cb_epoch, cb_csv, cb_rlr, cb_es_ft],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# 12) FINAL: QWK + distribution + confusion matrix\n",
        "# --------------------------\n",
        "y_true, y_pred = [], []\n",
        "for xb, yb_onehot in val_ds:\n",
        "    pr = model.predict(xb, verbose=0)\n",
        "    y_true.extend(np.argmax(yb_onehot.numpy(), axis=1).tolist())\n",
        "    y_pred.extend(np.argmax(pr, axis=1).tolist())\n",
        "\n",
        "final_qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
        "dist = np.bincount(np.array(y_pred), minlength=NUM_CLASSES)\n",
        "cm = confusion_matrix(y_true, y_pred, labels=list(range(NUM_CLASSES)))\n",
        "\n",
        "print(\"\\nFINAL QWK:\", round(final_qwk, 4))\n",
        "print(\"Prediction distribution:\", dist)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# final cm kaydet\n",
        "pd.DataFrame(cm, index=CLASS_NAMES, columns=CLASS_NAMES).to_csv(os.path.join(CM_DIR, \"cm_FINAL.csv\"))\n",
        "print(\"\\n Best model path:\", BEST_MODEL_PATH)\n",
        "print(\" History CSV:\", os.path.join(LOG_DIR, \"history.csv\"))\n",
        "print(\" Confusion matrices folder:\", CM_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7sMi2o84m_-",
        "outputId": "39e19215-6b46-4b71-ad77-d80a3f46845c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF: 2.19.0\n",
            "GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            " CKPT_DIR: /content/drive/MyDrive/Image Processing Project/checkpoints\n",
            " BEST_MODEL: /content/drive/MyDrive/Image Processing Project/checkpoints/best_model.keras\n",
            " CM_DIR: /content/drive/MyDrive/Image Processing Project/confusion_matrices_night\n",
            " Train label cleaned: 2930 -> 2930\n",
            " Val label cleaned  : 366 -> 366\n",
            " Train files: 2930 | missing: 0\n",
            " Val files  : 366 | missing: 0\n",
            " Resume: /content/drive/MyDrive/Image Processing Project/checkpoints/epoch_003_valloss_0.64533.weights.h5 (epoch=3)\n",
            "\n",
            "=== WARMUP (base frozen) ===\n",
            "\n",
            "=== FINE-TUNE (base trainable=True) ===\n",
            "\n",
            " Epoch progress: 1/10 (0.0%)\n",
            "Epoch 1/10\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655ms/step - acc: 0.5568 - auc: 0.7925 - loss: 1.0896 - top2_acc: 0.7076✅ QWK (val): 0.8556\n",
            "Confusion Matrix:\n",
            " [[171   1   0   0   0]\n",
            " [  1  25  12   0   2]\n",
            " [  5  11  72   7   9]\n",
            " [  0   2  11   1   8]\n",
            " [  0   1   9   1  17]]\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 0.59978, saving model to /content/drive/MyDrive/Image Processing Project/checkpoints/best_model.keras\n",
            "\n",
            "Epoch 1: saving model to /content/drive/MyDrive/Image Processing Project/checkpoints/epoch_001_valloss_0.59978.weights.h5\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 1s/step - acc: 0.5581 - auc: 0.7930 - loss: 1.0868 - top2_acc: 0.7088 - val_acc: 0.7814 - val_auc: 0.9157 - val_loss: 0.5998 - val_top2_acc: 0.8934 - val_qwk: 0.8556 - learning_rate: 1.0000e-04\n",
            "\n",
            " Epoch progress: 2/10 (10.0%)\n",
            "Epoch 2/10\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - acc: 0.7987 - auc: 0.9313 - loss: 0.5409 - top2_acc: 0.9259✅ QWK (val): 0.8665\n",
            "Confusion Matrix:\n",
            " [[171   1   0   0   0]\n",
            " [  4  17  18   0   1]\n",
            " [  6   2  85   7   4]\n",
            " [  0   0  12   6   4]\n",
            " [  0   1  12   2  13]]\n",
            "\n",
            "Epoch 2: val_loss improved from 0.59978 to 0.52852, saving model to /content/drive/MyDrive/Image Processing Project/checkpoints/best_model.keras\n",
            "\n",
            "Epoch 2: saving model to /content/drive/MyDrive/Image Processing Project/checkpoints/epoch_002_valloss_0.52852.weights.h5\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 187ms/step - acc: 0.7988 - auc: 0.9313 - loss: 0.5407 - top2_acc: 0.9258 - val_acc: 0.7978 - val_auc: 0.9352 - val_loss: 0.5285 - val_top2_acc: 0.9262 - val_qwk: 0.8665 - learning_rate: 1.0000e-04\n",
            "\n",
            " Epoch progress: 3/10 (20.0%)\n",
            "Epoch 3/10\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - acc: 0.8456 - auc: 0.9605 - loss: 0.4079 - top2_acc: 0.9539✅ QWK (val): 0.8550\n",
            "Confusion Matrix:\n",
            " [[171   1   0   0   0]\n",
            " [  2  19  16   0   3]\n",
            " [  4   5  86   3   6]\n",
            " [  0   0  10   3   9]\n",
            " [  0   3   9   1  15]]\n",
            "\n",
            "Epoch 3: val_loss improved from 0.52852 to 0.48642, saving model to /content/drive/MyDrive/Image Processing Project/checkpoints/best_model.keras\n",
            "\n",
            "Epoch 3: saving model to /content/drive/MyDrive/Image Processing Project/checkpoints/epoch_003_valloss_0.48642.weights.h5\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 201ms/step - acc: 0.8455 - auc: 0.9604 - loss: 0.4081 - top2_acc: 0.9538 - val_acc: 0.8033 - val_auc: 0.9422 - val_loss: 0.4864 - val_top2_acc: 0.9262 - val_qwk: 0.8550 - learning_rate: 1.0000e-04\n",
            "\n",
            " Epoch progress: 4/10 (30.0%)\n",
            "Epoch 4/10\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - acc: 0.8685 - auc: 0.9661 - loss: 0.3614 - top2_acc: 0.9660✅ QWK (val): 0.8613\n",
            "Confusion Matrix:\n",
            " [[171   1   0   0   0]\n",
            " [  2  23  13   0   2]\n",
            " [  5  10  88   1   0]\n",
            " [  0   2  14   2   4]\n",
            " [  0   3   9   1  15]]\n",
            "\n",
            "Epoch 4: val_loss did not improve from 0.48642\n",
            "\n",
            "Epoch 4: saving model to /content/drive/MyDrive/Image Processing Project/checkpoints/epoch_004_valloss_0.51380.weights.h5\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 169ms/step - acc: 0.8686 - auc: 0.9661 - loss: 0.3613 - top2_acc: 0.9660 - val_acc: 0.8169 - val_auc: 0.9375 - val_loss: 0.5138 - val_top2_acc: 0.9344 - val_qwk: 0.8613 - learning_rate: 1.0000e-04\n",
            "\n",
            " Epoch progress: 5/10 (40.0%)\n",
            "Epoch 5/10\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - acc: 0.9035 - auc: 0.9819 - loss: 0.2821 - top2_acc: 0.9800✅ QWK (val): 0.8843\n",
            "Confusion Matrix:\n",
            " [[171   1   0   0   0]\n",
            " [  1  22  15   0   2]\n",
            " [  3   8  84   6   3]\n",
            " [  0   0   4  13   5]\n",
            " [  0   3   7   2  16]]\n",
            "\n",
            "Epoch 5: val_loss did not improve from 0.48642\n",
            "\n",
            "Epoch 5: saving model to /content/drive/MyDrive/Image Processing Project/checkpoints/epoch_005_valloss_0.49269.weights.h5\n",
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 171ms/step - acc: 0.9035 - auc: 0.9819 - loss: 0.2821 - top2_acc: 0.9800 - val_acc: 0.8361 - val_auc: 0.9405 - val_loss: 0.4927 - val_top2_acc: 0.9372 - val_qwk: 0.8843 - learning_rate: 1.0000e-04\n",
            "\n",
            " Epoch progress: 6/10 (50.0%)\n",
            "Epoch 6/10\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - acc: 0.9223 - auc: 0.9887 - loss: 0.2226 - top2_acc: 0.9845✅ QWK (val): 0.8822\n",
            "Confusion Matrix:\n",
            " [[171   1   0   0   0]\n",
            " [  1  25  12   0   2]\n",
            " [  3  10  84   4   3]\n",
            " [  0   0   7   9   6]\n",
            " [  0   3   7   2  16]]\n",
            "\n",
            "Epoch 6: val_loss did not improve from 0.48642\n",
            "\n",
            "Epoch 6: saving model to /content/drive/MyDrive/Image Processing Project/checkpoints/epoch_006_valloss_0.50502.weights.h5\n",
            "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 182ms/step - acc: 0.9222 - auc: 0.9887 - loss: 0.2226 - top2_acc: 0.9845 - val_acc: 0.8333 - val_auc: 0.9391 - val_loss: 0.5050 - val_top2_acc: 0.9426 - val_qwk: 0.8822 - learning_rate: 5.0000e-05\n",
            "Epoch 6: early stopping\n",
            "Restoring model weights from the end of the best epoch: 3.\n",
            "\n",
            "FINAL QWK: 0.855\n",
            "Prediction distribution: [177  28 121   7  33]\n",
            "Confusion Matrix:\n",
            " [[171   1   0   0   0]\n",
            " [  2  19  16   0   3]\n",
            " [  4   5  86   3   6]\n",
            " [  0   0  10   3   9]\n",
            " [  0   3   9   1  15]]\n",
            "\n",
            " Best model path: /content/drive/MyDrive/Image Processing Project/checkpoints/best_model.keras\n",
            " History CSV: /content/drive/MyDrive/Image Processing Project/logs_night/history.csv\n",
            " Confusion matrices folder: /content/drive/MyDrive/Image Processing Project/confusion_matrices_night\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}